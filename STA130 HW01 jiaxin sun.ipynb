{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96346fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset from seaborn's built-in dataset\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the Titanic dataset:\\n\", titanic.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = titanic.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f'Number of rows: {villagers_df.shape[0]}')\n",
    "print(f'Number of columns: {villagers_df.shape[1]}')\n",
    "\n",
    "# Observations: an observation refers to row in the DataFrame, where each row represents a unique villager with various attributes, such as name, gender, species.\n",
    "# Variables: Variables are the columns in the dataset, representing the features of villagers (e.g., gender, species, personality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Summarize each column\n",
    "for column in villagers_df.columns:\n",
    "    print(f\"Summary of '{column}':\")\n",
    "    print(villagers_df[column].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731bdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_n           0\n",
      "id              1\n",
      "name            0\n",
      "gender          0\n",
      "species         0\n",
      "birthday        0\n",
      "personality     0\n",
      "song           11\n",
      "phrase          0\n",
      "full_id         0\n",
      "url             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = villagers_df.isnull().sum()\n",
    "\n",
    "# Show which columns have missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "#（1）.About the discrepancies between df.shape and df.describe:\n",
    "#df.shape: This function returns the overall dimensions of the dataset, specifically the number of rows (observations) and columns (variables). \n",
    "#For example, if the dataset contains 300 rows and 10 columns, will return , regardless of whether the columns are numeric or non-numeric.df.shape(300, 10)\n",
    "#df.describe(): This function provides summary statistics for numeric columns only by default. It will ignore non-numeric (categorical) columns\n",
    "#unless explicitly told to include them using . It summarizes only the numeric columns by providing information like the count, mean, standard deviation, etc.df.describe(include='all')\n",
    "\n",
    "#（2）.About the number of Columns Analyzed:\n",
    "#df.shape reflects all columns in the dataset (numeric and non-numeric).\n",
    "#df.describe() only includes the numeric columns, hence fewer columns are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "#(1)Attribute: attribute is a kind of data related to an object. it shows the feature of the object and it does not need parentheses when we use it. \n",
    "#An attribute provide information more easier to get and understand. It is accessed directly like a variable.\n",
    "\n",
    "#(2)Method: method is a kind of function which use to execute operations and computations. \n",
    "#It requires parentheses when we call it, in order to help it process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d20eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "#'count': The number of non-null observations in the dataset for each variable. It tells us how many data points are available.\n",
    "#'mean': the average value of the observations for each variable. \n",
    "#'std': the whole name of std is standard deviation.If it shows as a axes, the more concentrated the point distribution is,the std will be more higher. In conclusion, a higher standard deviation indicates more variability.\n",
    "#'min': it is the smallest value in the dataset, represent the minimum limit of the data range.\n",
    "#'25%': it represents the value below which 25% of the data points are found. It shows where the bottom quarter of the data lies.\n",
    "#'50%',This is the middle value of the dataset. Half of the data points are below this value, and half are above it. It gives an idea of the center of the data.\n",
    "#'75%': This is the value below which 75% of the data points are found. It shows where the top quarter of the data lies.\n",
    "#'max': This is the highest value in the dataset. It marks the upper end of the data range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a52c02e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (816063831.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    student_name   math_score   english_score   science_score\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "#(1)Use case: Assume I have a dataset of students' grades (df) with columns: student_name, math_score,and english_score. \n",
    "#The english_score column has many missing values. I want to analyze the data, when I use df.dropna() to remove only the rows that have missing values. \n",
    "#This is useful, rather than using del df['col']. because removing a few rows with missing values keeps the majority of the data. I will not lose other students data.\n",
    "\n",
    "#(2)use case: Assume I have a dataset of students' grades (df) with columns, there are many columns of english_score, math_score and number of good deed.\n",
    "#I want to analyze these data and select the best student. But this time the english_score losing over 90% of data,since the column is nearly empty, \n",
    "#it’s better to delete it altogether than risk skewing the analysis.\n",
    "\n",
    "#(3)Applying del df['col'] before df.dropna() can be important because it ensures that dropping columns with excessive missing data \n",
    "#before filtering out rows with just a few missing values. This prevents the potential loss of valuable rows that might only be missing data in the now-deleted column,\n",
    "#thereby maximizing the amount of usable data.\n",
    "\n",
    "#(4)\n",
    "student_name   math_score   english_score   science_score\n",
    "Alice          85           90              NaN\n",
    "Bob            78           NaN             NaN\n",
    "Carol          92           85              89\n",
    "Dave           NaN          76              91\n",
    "#Step 1: Analyze Missing Data: The science_score column has 2 missing values,some rows (like Bob's) have multiple missing values.\n",
    "\n",
    "#Step 2: use del df['science_score'] to remove the column because it has too many missing values. \n",
    "\n",
    "del df['science_score']\n",
    "\n",
    "#Step3: use df.dropna() to remove rows with any remaining missing values.\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Reason: Deleting the science_score column because it had too many missing values, making it unreliable for analysis.\n",
    "# Removing rows with missing values: After deleting the science_score column, we used df.dropna() to remove rows with any other \n",
    "# missing values (e.g., rows with missing math_score or history_score). This ensures that the final dataset is complete \n",
    "# and ready for analysis without gaps.\n",
    "\n",
    "# Before and after: Before cleaning: We had 4 rows and 3 columns.There were multiple missing values, particularly in the science_score column.\n",
    "# After cleaning: We now have 1 row and 2 columns. The dataset is complete, with no missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6ebc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Dataset before cleaning:\n",
      "  student_name  math_score  english_score  science_score\n",
      "0        Alice        85.0           90.0            NaN\n",
      "1          Bob        78.0            NaN            NaN\n",
      "2        Carol        92.0           85.0           89.0\n",
      "3         Dave         NaN           76.0           91.0\n",
      "\n",
      "Dataset after cleaning:\n",
      "  student_name  math_score  english_score\n",
      "0        Alice        85.0           90.0\n",
      "2        Carol        92.0           85.0\n"
     ]
    }
   ],
   "source": [
    "#7 here is the complete code below:\n",
    "\n",
    "!pip install pandas numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the initial dataset, using np.nan for missing values\n",
    "data = {\n",
    "    'student_name': ['Alice', 'Bob', 'Carol', 'Dave'],\n",
    "    'math_score': [85, 78, 92, np.nan],\n",
    "    'english_score': [90, np.nan, 85, 76],\n",
    "    'science_score': [np.nan, np.nan, 89, 91],\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset before cleaning\n",
    "print(\"Dataset before cleaning:\")\n",
    "print(df)\n",
    "\n",
    "# Delete the 'science_score' column due to too many missing values\n",
    "del df['science_score']\n",
    "\n",
    "# Remove rows with any remaining missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display the dataset after cleaning\n",
    "print(\"\\nDataset after cleaning:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baceba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "          count       mean        std     min       25%    50%        75%  \\\n",
      "embarked                                                                    \n",
      "C         168.0  59.954144  83.912994  4.0125  13.69795  29.70  78.500025   \n",
      "Q          77.0  13.276030  14.188047  6.7500   7.75000   7.75  15.500000   \n",
      "S         644.0  27.079812  35.887993  0.0000   8.05000  13.00  27.900000   \n",
      "\n",
      "               max  \n",
      "embarked            \n",
      "C         512.3292  \n",
      "Q          90.0000  \n",
      "S         263.0000  \n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "#8 \n",
    "#（1）df.groupby(\"col1\"): Groups the DataFrame df by unique values in the column col1. \n",
    "#This means all rows with the same value in col1 are put into the same group\n",
    "#[\"col2\"]: From each group created, this selects the col2 column.\n",
    "\n",
    "!pip install pandas numpy\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "embarked_fare_summary = df.groupby('embarked')['fare'].describe()\n",
    "print(embarked_fare_summary)\n",
    "\n",
    "#explain: df.groupby('embarked'): Groups the data by the embarked column \n",
    "#(which indicates the port of embarkation: C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "#['fare']: Focuses on the fare column (ticket price) for analysis.\n",
    "#.describe(): Provides summary statistics of the fare column for each embarkation port group, showing the distribution of ticket prices.\n",
    "\n",
    "#（2）Scope of Analysis:df.describe(): Provides a summary of statistics for each column in the entire DataFrame. \n",
    "#The count here represents the total number of non-missing values for each column across the whole dataset. \n",
    "#This gives a global view of data availability and distribution.df.groupby(\"col1\")[\"col2\"].describe(): Provides statistics for col2 within each group defined by col1. \n",
    "#The count in this context represents the number of non-missing values in col2 for each distinct value of col1. This gives a detailed view of data within specific groups.\n",
    "#Handling of Missing Values: df.describe(): The count reflects the overall impact of missing values on each column in the entire DataFrame.\n",
    "#Missing values affect the count, mean, and other statistics across the whole dataset.df.groupby(\"col1\")[\"col2\"].describe(): The count is specific to each group,\n",
    "#so it shows how missing values are distributed within each group rather than the entire dataset. Different groups may have different numbers of missing values,\n",
    "#which affects the count and other statistics for col2 within each group\n",
    "\n",
    "#（3）\n",
    "# A：\n",
    "#Fix the Error: To fix this, simply add the missing import statement at the beginning of your code:\n",
    "import pandas as pd\n",
    "\n",
    "# B：\n",
    "#Correct the Typo: Check your code and correct the typo in the file name to 'titanic.csv'.\n",
    "import pandas as pd\n",
    "# Correct URL or file name\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "# Perform analysis\n",
    "summary = df.describe()\n",
    "print(summary)\n",
    "\n",
    "#C:\n",
    "#Correct the Variable Name: Check your code to ensure that you are using the correct variable name. In this case, change DF to df.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Correct variable name\n",
    "grouped_summary = df.groupby(\"embarked\")[\"fare\"].describe()\n",
    "print(grouped_summary)\n",
    "\n",
    "#D\n",
    "#If your code should be pd.read_csv(url) but you accidentally write pd.read_csv(url, Python will raise a SyntaxError.\n",
    "#This is because the closing parenthesis is missing.\n",
    "\n",
    "#E\n",
    "#Mistyping a Function Name:\n",
    "#If your code should be df.groupby(\"col1\")[\"col2\"].describe(), but you mistakenly write df.group_by(\"col1\")[\"col2\"].describe(), \n",
    "#Python will raise an AttributeError because group_by is not a valid method for a DataFrame; the correct method is groupby.\n",
    "#Mistyping a Method Name:\n",
    "#Similarly, if your code should be df.groupby(\"col1\")[\"col2\"].describe(), but you mistakenly write df.groupby(\"col1\")[\"col2\"].describle(),\n",
    "#Python will raise an AttributeError because describle is not a valid method; the correct method is describe.\n",
    "\n",
    "#F\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Mistyped column name\n",
    "grouped_summary = df.groupby(\"Sex\")[\"age\"].describe()\n",
    "print(grouped_summary)\n",
    "\n",
    "KeyError: 'Sex'\n",
    "    \n",
    "#Check Column Names: Use df.columns to list all available columns in your DataFrame.\n",
    "#Update the Code: Ensure you use the exact column names from df.columns in both groupby and selection.\n",
    "\n",
    "\n",
    "#G\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Missing quotes around column name\n",
    "grouped_summary = df.groupby(sex)[\"age\"].describe()\n",
    "print(grouped_summary)\n",
    "\n",
    "NameError: name 'sex' is not defined\n",
    "\n",
    "#chatGPT give me a way to solve it:\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Correct use of quotes around column names\n",
    "grouped_summary = df.groupby(\"sex\")[\"age\"].describe()\n",
    "print(grouped_summary)\n",
    "\n",
    "#in my opinion, the chatgpt is more helpful to solve the problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deeab7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "          count       mean        std     min       25%    50%        75%  \\\n",
      "embarked                                                                    \n",
      "C         168.0  59.954144  83.912994  4.0125  13.69795  29.70  78.500025   \n",
      "Q          77.0  13.276030  14.188047  6.7500   7.75000   7.75  15.500000   \n",
      "S         644.0  27.079812  35.887993  0.0000   8.05000  13.00  27.900000   \n",
      "\n",
      "               max  \n",
      "embarked            \n",
      "C         512.3292  \n",
      "Q          90.0000  \n",
      "S         263.0000  \n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "embarked_fare_summary = df.groupby('embarked')['fare'].describe()\n",
    "print(embarked_fare_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019b1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
